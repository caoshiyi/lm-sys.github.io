---
title: "Recipe for Serving Thousands of Concurrent LoRA Adapters"
author: "Ying Sheng*, Shiyi Cao*, Dacheng Li, Coleman Hooper, Nicholas Lee, Shuo Yang, Christopher Chou, Banghua Zhu, Lianmin Zheng, Kurt Keutzer, Joseph E. Gonzalez, Ion Stoica"
date: "November 14, 2023"
previewImg: /images/blog/slora/serving_perf.png
---
In this blogpost, we introduce [S-LoRA](https://arxiv.org/abs/2311.03285) ([code](https://github.com/S-LoRA/S-LoRA)), a system designed for the scalable serving of many LoRA adapters. S-LoRA adopts the idea of

1. **Unified Paging** for KV cache and adapter weights to reduce memory fragmentation. 
2. **Heterogeneous Batching** of LoRA computation with different ranks leveraging optimized custom CUDA kernels which are aligned with the memory pool design.
3. **S-LoRA TP** to ensure effective parallelization across multiple GPUs, incuring minimal communication cost for the added LoRA computation compared to that of the base model. 

Evaluation results show that S-LoRA improves the throughput by up to 4 times and increase the number of served adapters by several orders of magnitude cpmpared to state-of-the-art libraries such as HuggingFace PEFT and vLLM (with naive support of LoRA serving).

## Introduction

The "pretrain-then-finetune" paradigm is commonly adopted in the deployment of large language models. Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method, is often employed to adapt a base model to a multitude of tasks, resulting in a substantial collection of LoRA adapters derived from one base model. Scalable serving of these many task-specific fine-tuned models is of crucial importance and offers the potential for large-scale customized LLM services. Below we briefly introduce how LoRA works and discuss about several of the design choices we met in practice for scalable serving of many concurrent LoRA adapters.

### Low-Rank Adaption (LoRA)

The motivation behind LoRA stems from the low intrinsic dimensionality of model updates during adaptation. In the training phase, LoRA freezes the weights of a pre-trained base model and adds trainable low-rank matrices to each layer. This approach significantly reduces the number of trainable parameters and memory consumption. When compared to full parameter fine-tuning, LoRA can often reduce the number of trainable parameters by orders of magnitude (e.g., 10000×) while retaining comparable accuracy.
Formally, for a pre-trained weight matrix $W\in \mathbb{R}^{h\times d}$, LoRA introduces the updates as $W' = W + AB$, where $A\in \mathbb{R}^{h\times r}$, $B\in \mathbb{R}^{r\times d}$, and the rank $r \ll \min(h,d)$. If the forward pass of a base model is defined by $h=xW$, then after applying LoRA, the forward pass becomes

$$
\begin{align}
h = xW' &= x(W+AB)
\\
&= xW + xAB.
\end{align}
$$

### `x(W + AB)` v.s. `xW + xAB`

One of the key innovations in the LoRA paper was the elimination of adapter inference latency by directly merging the adapter with the model parameters (as suggested by `Eq.(1)`). Additionally, to support multiple models on a single machine, the same paper proposes swapping adapters by adding and subtracting LoRA weights from the base model. While this approach enables low-latency inference for a single adapter and serial execution across adapters, it significantly reduces overall serving throughput and increases total latency when serving multiple adapters concurrently. We observe that the shared base model, which underpins numerous LoRA adapters, presents a substantial opportunity for batched inference. To achieve high-throughput multi-adapter serving, it is advantageous to separate the batchable base model computation from individual LoRA computations (as suggested by `Eq.(2)`).

<img src="/images/blog/slora/lora_serving_plot_num_adapters.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 50%"></img>
<p style="color:gray; text-align: center;">Figure 1: Ablation study comparing adapter merging and on-the-fly compute on A10G (24GB) with different α and number of adapters.</p>

In the above figure, we demonstrate a comparison between the two ways of perfoming the computation. For the adapter weights merging approach, we 1. update the abse model with current adapter weights before each new batch, and 2. switch to a new adapter if there are too many waiting requests.
We can see from the results that the merging method is efficient when there's only one adapter, outperforming the on-the-fly computation owing to a one-time merging cost.  However, its performance declines with more than 2 adapters, primarily because of the time-consuming switch between adapters. Such switching results in periods of GPU under-utilization. More adapters will lead to more frequent such switch and thus we believe that separating the computation for base model and LoRA addons should be the right choice for scalable LoRA serving.

### Reserved Memory v.s. Unified Memory

Another thing that needs to be figured out is how we should manage the memory for the adapters on GPU. One way to do this is to reserve some memory on GPU for adapter weights and smartly swap in & out the adapters from / to the host DRAM. Such method has certain limitations:

1. When the memory consumption of current active adapters is less than the reserved memory, we waste some memory that could be used for KV cache. This restriction ultimately reduces the attainable maximum batch size, leading to decreased throughput.
2. On the other hand, the reserved memory size can limit the maximum number of active adapters, which may result in insufficient requests for continuous batching and thus lower throughput.

Considering these factors, it is natural to consider a dynamic memory management scheme that can adjust the ratio of memory assigned to KV cache and adapter weights. A simple solution for this is to put them into the same pool and adopt the paging strategy, extending the idea of paged KV cache in [vLLM](https://github.com/vllm-project/vllm).

A KV cache tensor for a request in a layer has a shape of `(S, H)`, where `S` denotes the sequence length and `H` represents the hidden dimension of the served model. The shape of a LoRA weights is `(R, H)` with `R` standing for the rank and `H` the hidden dimension. Notably, both `S` and `R` varies. From here we can observe that `H` is a common factor of all these different object sizes. Therefore, by setting the page size to be `H` in the memory pool we can significantly reduce the memory fragmentation and ease the memory management on a large scale.

### Non-contiguous Memory Layout

As a result of our unified memory pool, the KV caches and adapter weights are stored interleaved and non-contiguously, as shown in the figure below.

<img src="/images/blog/slora/unified_memory_pool.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 50%"></img>
<p style="color:gray; text-align: center;">Figure 2: KV cache and Adapter Weights Layout in the Unified Memory Pool.</p>

One challenge of non-contiguous memory layout for KV cache and adapter weights is that we cannot utilize the high-performance operators provided in popular libararies such as Pytorch and xFormers, as they all require the tensors lie in contiguous memory. For paged attention, we utilize [LightLLM](https://github.com/ModelTC/lightllm)'s implementation for TokenAttention. For paged LoRA computation, [CUTLASS](https://github.com/NVIDIA/cutlass) provides high-performance Grouped Gemm kernels, but it still requires the contiguous memory layout for each adapter's weights. Therefore we implemented customized kernels for our memory pool. In the prefill stage, for each request the kernel handles a sequence of tokens and gathers adapter weights with different ranks from the memory pool. We implemented it in Triton with tiling. In the decode stage, for each request the kernel handles a single token and gathers adapter weights with different ranks from the memory pool. It is modified from [Punica](https://github.com/punica-ai/punica)'s BGMV kernel to support multiple ranks in a batch and more fine-grained memory gathering to align with our memory pool design.

### Scale Beyond one GPU - Tensor Parallelism

Tensor parallelism is the most widely used parallelism method since its single-program multiple-data pattern simplifies its implementation and integration with existing systems. Tensor parallelism can reduce the per-GPU memory usage and latency when serving large models. In our setting, the additional LoRA adapters introduce new weight matrices and matrix multiplications, which calls for new partition strategies for these added items.

The base model uses the [Megatron-LM](https://arxiv.org/abs/1909.08053) tensor parallelism strategy, our approach aims to align the partition strategies of inputs and outputs of the added LoRA computation with those of the base model. We further minimize the communication costs by avoiding unnecessary communications and fusing some of the communications.

<img src="/images/blog/slora/lora_tp.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: %"></img>
<p style="color:gray; text-align: center;">Figure 3: Tensor parallelism partition strategy for batched LoRA computation.</p>

The figure above demonstrates the tensor parallelism partition strategy for batched LoRA computation. This is a computational graph where nodes represent tensors/operators and the edges represent dependency. We use different colors to represent different partition strategies, which include column partition, row partition, partial sum, and replication. The per-GPU shape of each tensor is also annotated in gray. Note that $B$ is the number of tokens, $h$ is the input dimension, $N$ is the number of devices, $d$ is the hidden size, and $r$ is the adapter rank.

## Methods Summary

1. **Unified Paging**: To reduce memory fragmentation and increase batch size, S-LoRA introduces a unified memory pool. This pool manages dynamic adapter weights and KV cache tensors by a unified paging mechanism.
2. **Heterogeneous Batching**: To minimize the latency overhead when batching different adapters of varying ranks, S-LoRA employs highly optimized custom CUDA kernels. These kernels operate directly on non-contiguous memory and align with the memory pool design, facilitating efficient batched inference for LoRA.
3. **S-LoRA TP**: To ensure effective parallelization across multiple GPUs, S-LoRA introduces a novel tensor parallelism strategy. This approach incurs minimal communication cost for the added LoRA computation compared to that of the base model. This is realized by scheduling communications on small intermediate tensors and fusing the large ones with the communications of the base model.

## Evaluation

### Model Settings

| Setting | Base model | Hidden size | Adapter ranks   |
| ------- | ---------- | ----------- | --------------- |
| S1      | Llama-7B   | 4096        | {8}             |
| S2      | Llama-7B   | 4096        | {64, 32, 16, 8} |
| S4      | Llama-13B  | 5120        | {64, 32, 16}    |
| S5      | Llama-30B  | 7168        | {32}            |
| S6      | Llama-70B  | 8192        | {64}            |

### Baselines

We compare S-LoRA with HuggingFace PEFT and vLLM.

1. PEFT stands for HuggingFace PEFT: We build a server using it that batches single adapter requests and switches adapter weights between batches.
2. vLLM-packed: Since vLLM does not support LoRA, we merge the LoRA weights into the base model and serve the multiple versions of the merged weights separately. To serve m LoRA adapters, we run `m` vLLM workers on a single GPU, where multiple workers are separate processes managed by NVIDIA MPS.

### Results

The figure below shows the throughput (req/s) comparison between S-LoRA, vLLM-packed, and PEFT. The hardware is a single A100 (80GB). We run PEFT for a shorter duration when $n=100$. We do not evaluate PEFT for $n\geq 1000$, as its throughput is already very low for a small $n$. ``red cross'' denotes out-of-memory.

<img src="/images/blog/slora/serving_perf.png" style="display:block; margin-top: auto; margin-left: auto; margin-right: auto; margin-bottom: auto; width: 90%"></img>
<p style="color:gray; text-align: center;">Figure 4: Performance comparison between S-LoRA, vLLM-packed, and PEFT.</p>

Remarkably, S-LoRA can serve 2,000 adapters simultaneously, maintaining minimal overhead for the added LoRA computation. In contrast, vLLM-packed needs to maintain multiple weight copies and can only serve fewer than 5 adapters due to the GPU memory constraint. The throughput of vLLM-packed is also much lower due to the missed batching opportunity. Overall, S-LoRA achieves a throughput up to **4x** higher than vLLM-packed when serving a small number of adapters, and up to **30x** higher than PEFT, while supporting a significantly larger number of adapters.

Please check our [paper](https://arxiv.org/abs/2311.03285) for more results on S-LoRA variants and the scalability of S-LoRA TP.

## Citation

```bibtex
@misc{sheng2023slora,
      title={S-LoRA: Serving Thousands of Concurrent LoRA Adapters}, 
      author={Ying Sheng and Shiyi Cao and Dacheng Li and Coleman Hooper and Nicholas Lee and Shuo Yang and Christopher Chou and Banghua Zhu and Lianmin Zheng and Kurt Keutzer and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2311.03285},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
```
